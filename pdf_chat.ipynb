{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load pdf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Reads the text content from a PDF file and returns it as a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The file path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    - str: The concatenated text content of all pages in the PDF.\n",
    "    \"\"\"\n",
    "    # Logic to read pdf\n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    # Loop over each page and store it in a variable\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = load_pdf(file_path=\"D:\\\\chatbot\\\\pdf-query\\\\rag-pdf-chat\\\\We.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store knowledge in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasha\\AppData\\Roaming\\Python\\Python312\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "\n",
    "index_name = \"docs\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024, \n",
    "        metric=\"cosine\", \n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\", \n",
    "            region=\"us-east-1\"\n",
    "        ) \n",
    "    ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk the content based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Chunk the document based on h2 headers.\n",
    "markdown_document = pdf_text\n",
    "headers_to_split_on = [\n",
    "    (\"##\", \"Header 2\")\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "# Initialize a LangChain embedding object.\n",
    "model_name = \"multilingual-e5-large\"  \n",
    "embeddings = PineconeEmbeddings(  \n",
    "    model=model_name,  \n",
    "    pinecone_api_key=os.environ.get(\"PINECONE_API_KEY\")  \n",
    ")  \n",
    "\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=md_header_splits,\n",
    "    index_name=\"docs\",\n",
    "    embedding=embeddings, \n",
    "    namespace=\"wondervector5000\" \n",
    ")\n",
    "\n",
    "time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Pineconeâ€™s list and query operations to look at one of the records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)\n",
    "namespace = \"wondervector5000\"\n",
    "\n",
    "for ids in index.list(namespace=namespace):\n",
    "    query = index.query(\n",
    "        id=ids[0], \n",
    "        namespace=namespace, \n",
    "        top_k=1,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1\n",
      "\n",
      "Chat with knowledge:\n",
      "I don't have enough context to provide a specific answer. Could you please provide more details or specify which information you are referring to?\n",
      "\n",
      "Chat without knowledge:\n",
      "The main purpose of the information given is to provide knowledge, answer questions, or convey a message to the reader or audience. It may also be intended to educate, inform, persuade, entertain, or inspire. Ultimately, the purpose of the information will depend on the context in which it is presented and the goals of the communicator.\n",
      "\n",
      "Query 2\n",
      "\n",
      "Chat with knowledge:\n",
      "I don't know the answer to that question.\n",
      "\n",
      "Chat without knowledge:\n",
      "If the Neural Fandango Synchronizer is giving you a headache, it is important to stop using it immediately and give yourself a break. Take some time to rest and relax, drink plenty of water, and consider taking over-the-counter pain medication if needed. If the headache persists or worsens, it is recommended to consult a healthcare professional for further advice and guidance. Additionally, you may want to consider adjusting the settings or usage of the Neural Fandango Synchronizer to see if that helps alleviate the headache.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA \n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize a LangChain object for chatting with the LLM\n",
    "# without knowledge from Pinecone.\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=os.environ.get('OPENAI_API_KEY'),\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Initialize a LangChain object for chatting with the LLM\n",
    "# with knowledge from Pinecone. \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever()\n",
    ")\n",
    "\n",
    "# Define a few questions about the WonderVector5000.\n",
    "query1 = \"\"\"What is main purpose of information given?\"\"\"\n",
    "\n",
    "query2 = \"\"\"The Neural Fandango Synchronizer is giving me a \n",
    "headache. What do I do?\"\"\"\n",
    "\n",
    "# Send each query to the LLM twice, first with relevant knowledge from Pincone \n",
    "# and then without any additional knowledge.\n",
    "print(\"Query 1\\n\")\n",
    "print(\"Chat with knowledge:\")\n",
    "print(qa.invoke(query1).get(\"result\"))\n",
    "print(\"\\nChat without knowledge:\")\n",
    "print(llm.invoke(query1).content)\n",
    "print(\"\\nQuery 2\\n\")\n",
    "print(\"Chat with knowledge:\")\n",
    "print(qa.invoke(query2).get(\"result\"))\n",
    "print(\"\\nChat without knowledge:\")\n",
    "print(llm.invoke(query2).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
